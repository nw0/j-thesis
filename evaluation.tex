\documentclass[thesis.tex]{subfile}

\chapter{Evaluation}
\label{ch:evaluation}

This chapter is devoted to the performance evaluation of our implementation of time-tiling in Devito.
It is organised into a number of sections:

\begin{itemize}
	\item First, we recall the motivation of this project, and the objectives of our evaluation (Section~\ref{sec:eval-objs}).
	\item We then discuss the restrictions that time-tiling imposes on our use of Devito, and the considerations that it warrants (Section~\ref{sec:time-tiling-details}).
	\item Next, we present the test methodology we have adopted (Section~\ref{sec:test-method}), and introduce the \emph{roofline model} (Section~\ref{sec:roofline-intro}), which is widely used to understand the performance of stencil calculations.
	We also discuss the parameters varied in the tests (Section~\ref{sec:eval-params}).
	\item Sections~\ref{sec:perf-laplace} and~\ref{sec:perf-awe} examine the performance of two families of stencils, generated by the Laplace and acoustic wave equation operators respectively.
	\item Finally, we discuss limitations of our performance testing (Section~\ref{sec:perf-limitations}), and consider further work in evaluation (Section~\ref{sec:further-eval}).
\end{itemize}


\section{Objective}
\label{sec:eval-objs}

The motivation for this project is to reduce computational runtime by exploiting data locality through time-tiling.
We have described the motivation and investigated tools which have achieve performance gains from the technique.
It had previously been shown that Devito benefits from reductions in runtime of up to 27.5\% with time-tiling using CLooG~\cite{dylan}.

Chapter~\ref{ch:implementation} described our implementation of time-tiling in Devito.
The objectives of this evaluation are as follows:

\begin{itemize}
	\item Measure the performance gain against non-tiled and spatially-tiled computations;
	\item Verify any requirements to perform time-tiling effectively;
	\item Determine procedures and heuristics to identify the best-performing parameters for time-tiling;
	\item Additional verification of correctness, and margin of error introduced against non-tiled code\footnote{For example, due to non-associativity of floating-point arithmetic.} if any.
\end{itemize}


\section{Details for evaluating time-tiling}
\label{sec:time-tiling-details}

As time-tiling has been implemented within Devito, there is no need for any external tool such as a polyhedral compiler.
Nevertheless, the following topics warrant careful consideration when applying and analysing time-tiling.

\subsection{Time-buffering}
\label{sec:time-buffering}

% Time buffering and save=n>2*time tile size (a nice diagram!)
Time-buffering is a memory-saving technique.

\begin{figure}[!ht]
\begin{lstlisting}
for (int t = t_s; t < t_e; t++)
  for (int x = x_s; x < x_e; x++)
    A[t][x] = A[t-1][x] + A[t-2][x] + ... + A[t-n][x];
\end{lstlisting}
	\caption{A stencil with a value depending on data in the previous \(n\) time iterations. \(n\) is usually small compared to the problem domain, (\texttt{t\_e - t\_s + n}) here.}
	\label{lst:buffer-eg}
\end{figure}

A stencil may only have dependencies on data from finitely many previous time iterations.
Consider a stencil with a data dependence on the last \(n\) time iterations (Figure~\ref{lst:buffer-eg}).
It is clear that whenever \(t > n\), data from the 0-th time iteration is no longer required, and can be overwritten.

When space-tiling in Devito, each time iteration only begins once the previous iteration has finished.
Therefore, we only need storage for \(n+1\) time iterations of data: the current iteration being computed; and the previous \(n\) iterations, its dependencies.

\wip{Using time buffering for AWE}
To allow for rigorous correctness checks (Section~\ref{sec:eval-func-corr}), we have \emph{not} used time-buffering during the evaluation.
Nevertheless, it is straightforward to establish bounds on the size of a time buffer.
It is clear that for time-tiling to perform better than spatial tiling, at least \(n+2\) time iterations must be stored.
\footnote{From a synchronisation perspective, this would be needlessly challenging. It is a fairly simple exercise to show that time-tiling needs at most \(n+t\) time iterations of storage, where \(t\) is the size of a time tile. We would recommend this approach.
\clar{Should this be in a footnote? Need diagram/proof?}}


\subsection{Auto-tuner iterations}
% Further prereqs to use the AT
Previously covered in Section~\ref{sec:autotune}, the Devito auto-tuner experimentally searches for tile sizes achieving the lowest runtime.
To do this, it needs to store some time iterations of computations: this value is governed by a parameter.\footnote{\wip{var naming is hard} The \texttt{at\_squeezer} parameter is the number of iterations that are computed, therefore requiring \(n + \texttt{at\_squeezer}\) time iterations of storage.}

For its results to be meaningful, we need to compute enough time iterations to distinguish between large time tile sizes.
Without time-tiling, a fairly small number of time iterations (4) sufficed for auto-tuning.
In our evaluation, we decided that the largest desirable time tile size was 16, to which we set the parameter.

\subsection{Time-tiling parameters}
\wip{What arguments; define skewing factor; redundant? discussed later}

Time-tiling provides more parameters: a valid skewing factor, and a tile size for the time dimension.
The tile size search was integrated into the auto-tuner, as it already had functionality to search for spatial tile sizes.
Arguments on data alignment give that the minimum valid skewing factor would produce the lowest runtime computation.
It was decided to try skewing factors by hand, as their validity would depend on the stencil (Section~\ref{sec:bg-skewing}).
We have found that that the data alignment argument holds for the experiments that we have performed (see Appendix TODO for the complete results).

\subsection{Arithmetic intensity under time-tiling}
\label{sec:arithmetic-intensity}

Recall that \emph{arithmetic intensity} is defined to be the number of floating point operations performed per byte.
Time-tiling is an optimisation to improve the re-use of cached data, effectively increasing the arithmetic intensity of a stencil, as it needs to be fetched from memory less frequently.

Suppose a stencil requires data from the last \(n\) time iterations.
Devito reports arithmetic intensity as `operational intensity', and calculates it as follows:

\begin{description}
	\item[Non-tiled code] Arithmetic intensity is calculated from the definition, assuming that data that has been loaded into the cache would \emph{not} need to be re-loaded in that time iteration.
	In reality, with a sufficiently large iteration space, each byte would have to be loaded into cache more than once for each time iteration.
	The measure therefore \emph{overestimates} the number of floating point operations per byte loaded from memory and hence true arithmetic intensity.
	In the graphs later in this chapter, this would result in a \emph{right shift} of a data point from its true location.

	\item[Space-tiled code] Again, Devito assumes data need only be loaded into the cache once per time iteration, and that it would be used fully before being evicted, calling this scheme `compulsory memory traffic'.
	For each time iteration, this requires the previous \(n\) time iterations of data to be loaded from memory.
	This produces the same figure for arithmetic intensity as the measure for non-tiled code above, if the stencil only requires data from the previous time iteration, as data needs to be loaded into the cache again for each time iteration.
	Since the dependencies of tiles overlap in previous time iterations, with a sufficiently large iteration space, some dependencies will need to be loaded into cache more than once (assuming at least two spatial dimensions with non-zero space order).\footnote{This is fairly simple to see. Ignore all but two spatial dimensions with non-zero space order, and suppose otherwise for a contradiction. Arbitrarily choose a finite cache size of \(\Omega\) floating-point numbers, and call our spatial dimensions \texttt{x}, \texttt{y} with dimension extents \(x, y\) respectively. W.l.o.g.~\(x \le y\) and that the stencil has space order \(d>0\) in both dimensions (take the minimum). Now we need \(xd \le \Omega\), since minimising dependency overlap along the \texttt{x} dimension demands that we calculate the first \texttt{x} `row' before any other \texttt{x}-tiles. But we can choose \(x > \Omega / d\).}
	Thus this measure also \emph{overestimates} arithmetic intensity, but to a smaller extent that can be estimated.\footnote{To see that the bound is realistic, consider `row-by-row' tile scheduling as in the previous proof, and spatial dimensions \texttt{x1}, ..., \texttt{xn}, with extents \(x_1, ..., x_n\), space orders \(d_1, ..., d_n\), and chosen tile sizes \(b_1, ..., b_n\). Can be shown that the data the \(x_i d_i\) points from each \texttt{xi}-row need be loaded into cache at most twice. Since an \texttt{xi}-row spans \(b_i\) iterations, at most \(d_i / b_i\) points are loaded into cache twice for the \texttt{xi}-dimension. An easy addition exercise gives an overall bound for the iteration space, and the measure under spatial tiling is clearly a tighter estimate than the measure under non-tiled computation.}
\end{description}

Remark: later, we will see that it is preferable to overestimate the arithmetic intensity of a stencil computation than underestimate it, and we provides arguments for why the measures stated in this section are useful and fairly realistic.
This will be explained in detail when studying the roofline model in Section~\ref{sec:roofline-intro}, as it requires other ideas that have yet to be introduced.

We need to establish a new measure of arithmetic intensity under time-tiling, preferably one compatible with the measure under spatial tiling.
We consider the following statements to follow naturally from Devito's calculation of arithmetic intensity under spatial tiling.

\begin{itemize}
	\item Analogously to space-tiling, we assume that data will be used fully before being evicted from cache.
	\item If \(t\) is the time tile size, we can calculate \(t\) time iterations of data from \(n\) previous time iterations loaded into the cache, as opposed to calculating 1 time iteration from the same data.
	\item Thus we have performed \(t\) times as many floating-point operations on it.
\end{itemize}

\begin{framed}
Therefore, we propose that the \emph{arithmetic intensity of a stencil computation under time-tiling} be defined to be the \textbf{arithmetic intensity of the same computation under spatial tiling multiplied by the size of the time tile used.}
\end{framed}

This is consistent with the measure under spatial tiling, since a spatially tiled loop is similar a time-tiled version of the loop under a (legal) interchange of the time and spatial tile loops.
Note that the previous argument on spatial tiling shows that this measure will also overestimate the arithmetic intensity, again exacerbated if the space order is large.
Also note that the same computation as seen previously to bound this overestimate will work, considering time as another dimension with order as the time order.
However, the bounds will be correspondingly wider.\footnote{Observe that the addition of another tiled dimension is likely to shrink the tile volume (i.e.~the product of tile sizes in all dimensions) in the other dimensions, widening the bound. This applies equally to the previous calculation.}


\section{Testing methodology}
\label{sec:test-method}

Realistic test cases vary from those that are more memory-intensive to those that are more computationally intensive on a given hardware configuration.
Under the former regime, computational (arithmetic) intensity is fairly low, and the CPU uses data faster than it can be transferred from memory; in the latter case, data can be transferred more rapidly than it can be utilised.

A key premise of Devito is that arithmetic intensity can be decreased at the cost of higher memory pressure, by manipulating expressions~\cite{fabio-memory}.
Time-tiling reduces memory pressure by increasing reuse of cached data before it is evicted.\footnote{However, this effectively increases the arithmetic intensity, as discussed in this chapter.}
Accepting the premise that arithmetic intensity can be reduced sufficiently, time-tiling can be used to full effect in reducing memory pressure.\footnote{See Section~\ref{sec:roofline-intro} (``Roofline model'') for more detail.}
Therefore, the most relevant test cases to time-tiling are those which are bound by memory throughput.

\subsection{Hardware and software environment}
Evaluation was performed on a machine equipped with a single Intel Xeon\textregistered E5-2470 operating at 2.30 GHz, with 8 cores and 16 threads, and 64 GB of DRAM.
It ran Ubuntu 16.04 LTS, with all running services required either for the operating system or our evaluation, to minimise external effects on runtime.

\clar{Discuss cache?}

To ensure a realistic testing environment and utilise all the available resources, the following hold throughout the experiments:

\clar{Assume reader knows about OpenMP?}

\wip{Explicit variables, check definitions in references}
\begin{itemize}
	\item OpenMP directives enabled in Devito. Indicates that a loop should be computed in parallel (typically the body of the inner time loop).
	\item OpenMP environment variable to utilise all 16 available threads.
	\item OpenMP environment variable not to migrate threads (``thread pinning''), as well as allocate threads to different cores. Thread migration incurs a significant performance overhead, and parallelism would not help if all threads executed in serial on one core. \clar{Too brief?}
	\item Usage of the Intel C Compiler, \texttt{icc}.
	\item Devito enables the highest level of compiler optimisation and parallelism in the compiler by default.
\end{itemize}

\subsection{Use of auto-tuner}
% Usage of AT -- parameters (skewing factor, time and space tile sizes)
We were wary of assuming which tile sizes might be optimal for time-tiling, as they are not easily predicted from those optimal for spatial tiling or cache size~\cite{lam91}.
Therefore, the extended Devito auto-tuner (Section~\ref{sec:autotune}) was used to explore as many plausible tile sizes combinations as possible.

As detailed above, experimentation on the skewing factor was done manually.
An exhaustive search was performed, as the space was relatively small, and the effect on our stencils not well studied.

\wip{For Laplace only}
As a matter of expedience, the auto-tuner was run for at least 3 trials.
If these produced the same combination of tile sizes, this combination would be chosen as optimal, and run repeatedly to determine its runtime.
This obviated the need to run auto-tuning for each data point, reducing the time needed to obtain a data point by 65\%.

\subsection{Functional correctness}
\label{sec:eval-func-corr}

In addition to new test cases, to build confidence in correctness of the newly-implemented skewing and tiling transformations, each application of time-tiling was numerically verified against a non-tiled computation.

\wip{Keep this box until results finalised: result equality claim}

In all experiments, the results were discovered to be equal.\footnote{We refer to equality under floating-point comparison; this is \emph{bitwise equality}.}
Since floating-point arithmetic is non-associative, the natural implication is that the resulting value in each field was reached through the same expressions, although the computations had been re-ordered.
Additional checks were used to determine that the results were not merely zeroes, or diverging to infinity.

\subsection{Reporting of runtimes}
Wherever runtimes have been collected in this evaluation, the reported figure will be the \emph{minimum} of the runtimes collected in trials.
This is taken as most representative, as any noise on the testing machine is minimised, since outside factors can only increase the runtime of our computations.


\section{Roofline model}
\label{sec:roofline-intro}

The roofline model describes how arithmetic (or \emph{operational}) intensity kernel affects its performance on a given system.
In particular, it gives upper bounds for performance based on memory bandwidth and CPU performance, and describes the bottlenecks that a program would encounter based on its arithmetic intensity~\cite{roofline}.
The model states that as arithmetic intensity increases, performance increases at a rate determined by the memory bandwidth, until it reaches the performance limit (in Flops, floating-point operations per second) of the processor.

As a primitive, since a stencil computation on a grid of a particular size requires a fixed number of floating-point operations, we want to maximise this number.

\subsection{Bounds of the test machine}
The Sandy Bridge architecture supports execution of up to 16 single-precision floating point operations per cycle, giving a theoretical maximum of 294.4 GFlops.\footnote{Calculated as 16 floating-point operations/cycle \(\times 2.3 \times 10^9\) cycles/sec/core \(\times\) 8 cores (on a single-node machine) \(= 294.4\) GFlops.
\\\wip{Fabio: 2.3GHz is the `processor base frequency', which I suspect is still not what you mean? Pl. confirm.}}
The \emph{LINPACK} benchmark~\cite{linpack}, highly optimised and likely to perform faster than any stencil we evaluate, achieves a maximum of 131 GFlops on this machine; it is standard practice to double this figure to obtain the maximum for single-precision floating-point operations, giving a practical bound of 262 GFlops in our test scenarios.
The \emph{STREAM} benchmark~\cite{stream} indicates that the peak memory bandwidth of the machine is 17.3 GB/s.

\subsection{Arithmetic intensity and cache reuse}
Figure~\ref{fig:roofline-benchmark} shows and explains the bounds of a stencil of a given arithmetic intensity.
A given stencil computation can be plotted as a point on the graph, determined by its GFlops achieved, which we measure at runtime, and its inherent arithmetic intensity.
Clearly, these are strict bounds on the performance of a stencil.

\begin{figure}[!ht]
\centering
\begin{tikzpicture}
\begin{axis}[
legend style={at={(0.95,0.05)},anchor=south east},
xlabel={Arithmetic intensity},
ylabel={GFlops achieved},
ytick distance=25,
]
% roofline
\addplot [domain=4:15.15,name path=A] {17.3*x};
\addplot [domain=15.15:25] {262.01};
% boundary
\path[name path=X] (4, 0) -- (200, 0);
\addplot[gray, pattern=north west lines] fill between[of=A and X, soft clip={domain=4:15.15}];
\addplot +[mark=none,dashed] coordinates {(15.15, 69.2) (15.15, 259.5)};
\end{axis}
\end{tikzpicture}
\caption{Roofline graph for the test machine. Performance of any stencil (GFlops) of given arithmetic intensity cannot exceed the solid lines plotted. A stencil falling into the shaded region (arithmetic intensity < 15.15) will be bounded by memory bandwidth indicated by the sloped line, and those in unshaded region (arithmetic intensity > 15.15) are bounded by processor performance, the horizontal line. These two solid lines are the \emph{roofline}.}
\label{fig:roofline-benchmark}
\end{figure}

We previously suggested that it was preferable to overestimate arithmetic intensity than underestimate it.
As we can see from the graph, if we permit ourselves to underestimate arithmetic intensity, a point could lie \emph{above} the roofline, contradicting the model.
Therefore, we must not underestimate arithmetic intensity.

Recall that the objective of time-tiling is to maximise cache reuse by exploiting data locality between time iterations.
As previously argued, if a computation is bounded by memory throughput (hence in the shaded region), improving effective throughput through more effective cache usage could be reasonably argued to decrease the runtime.
From the previous discussion (Section~\ref{sec:arithmetic-intensity}), we established that tiling transformations effectively increase the arithmetic intensity of a stencil computation, resulting in a rightward shift of a data point on this graph.

We see from the diagram that such a rightward shift starting from any point in the shaded region increases the performance bound under the roofline.
Therefore, we seek to demonstrate that this is accompanied by an upward shift (combined, a diagonal shift) of the performance of a memory-bounded stencil computation.
In particular, we hypothesise that the performance increase of stencils that are more memory bounded (lower arithmetic intensity) will yield the greatest benefit from time-tiling.


\section{Test parameters}
\label{sec:eval-params}

\paragraph{Skewing factor}
The skewing factor was varied to understand the relationship between skewing factor and runtime.
In particular, we wished to determine if the minimum legal skewing factor would result in the minimum runtime as hypothesised.

\paragraph{Space order}
The space order of the computation determines the precision of the result.
In solving differential equations, is the order of the approximation, beyond which smaller terms are discarded.
A higher space order means each cell is computed using more of its neighbours, resulting in greater precision.
This would require a larger stencil, and hence a larger skewing factor.

\paragraph{Domain size}
The size of the domain is the product of all four dimensions in the simulation.
As memory was limited to 64GB needed to store both time-tiled results and non-tiled results for numerical verification, we decided to use shorter time dimensions, maximising the spatial dimension size for realism.
We justify this choice, reasoning that runtime should be proportional to the number of time tiles executed.\footnote{This was verified using smaller spatial dimensions.}
Therefore, we chose time dimensions 16 or 32, as these would be small multiples of the time tile size.


\section{Performance of the Laplace operator}
\label{sec:perf-laplace}

\subsection{Application of the operator}
The Laplacian is the operator giving the divergence of the gradient of a scalar function, commonly used in mechanics.
In our evaluation, we used three spatial dimensions and a time dimension, with a deterministically generated input domain for numerical verification.
The input domain was chosen to avoid divergence of values, again for the purposes of numerical verification.

We previously stated that we were studying a family of stencils generated by this operator.
Devito makes it straightforward to generate stencils of varying space order, otherwise a non-trivial task; we have considered the Laplace operator with space orders 2, 4, 8, and 16.
% Sample code from apply.py
% Insert perf1.py into appendix (invocation of the operator, testing rigour)

\subsection{Results}
% Sample program output

\begin{table}[!ht]
\centering
\begin{tabular}{rr|cccc|cccc}
\toprule
& Grid size & \multicolumn{4}{c}{\( 32 \times 500^3 \)} & \multicolumn{4}{c}{\( 16 \times 600^3 \)} \\
& Space-order & 2 & 4 & 8 & 16 & 2 & 4 & 8 & 16 \\
\midrule
N & \footnotesize runtime (s) & 6.121 & 7.572 & 10.65  & 16.941 & 5.189 & 6.402 & 9.026 & 14.87 \\
S & \footnotesize runtime (s) & 3.964 & 3.760 & 4.060 & 5.778 & 3.489 & 3.376 & 3.684 & 4.980 \\
% & tile size & 32,64 & 128,8 & 128,8 & 16,16 & 32,40 & 32,32 & 8,64 & 16,16 \\
T & \footnotesize runtime (s) & 2.939 & 2.957 & 3.762 & 5.718 & 2.712 & 2.790 & 3.288 & 4.929 \\
& \footnotesize decrease (\%) & 25.9\% & 21.4\% & 7.34\% & 1.04\% & 22.3\% & 17.4\% & 10.8\% & 1.02\% \\

\midrule
(t,8) & \footnotesize runtime (s) & 3.348 & 3.257 & 3.762 & 5.718 & 3.089 & 2.966 & 3.341 & 4.929 \\
% & tile size & 16,32,64 & 16,32,64 & 16,32,64 & 1,16,16 & 16,64,24 & 16,32,64 & 16,16,128 & 1,16,32 \\
(t,4) & \footnotesize runtime (s) & 3.114 & 3.090 & 3.764 & - & 2.844 & 2.811 & 3.288 & - \\
% & tile size & 16,32,64 & 16,32,64 & 8,16,16 & & 16,48,24 & 8,16,16 & 16,32,64 & \\
(t,2) & \footnotesize runtime (s) & 2.956 & 2.957 & - & - & 2.788 & 2.790 & - & - \\
% & tile size & 16,32,64 & 16,32,64 & & & 8,16,32 & 8,16,16 & & \\
(t,1) & \footnotesize runtime (s) & 2.939 & - & - & - & 2.712 & - & - & - \\
% & & 16,32,64 & & & & 16,32,32 & & & \\
\bottomrule
\end{tabular}
\caption{Runtimes from using the Laplace operator. N, S, T represent minimum runtimes without tiling, with spatial tiling, and with time-tiling respectively. (t,\(k\)) indicates results for time-tiling with a skewing factor of \(k\); T indicates the minimum of these. Decrease is from spatial tiling to time-tiling. Only valid skewing factors used.}
\label{tab:laplace-results}
\end{table}

\begin{figure}[!ht]
\centering
\begin{tikzpicture}
\begin{axis}[
legend style={at={(0.95,0.05)},anchor=south east},
xlabel={Space order},
ylabel={Runtime(s)},
ytick distance=0.5,
]
\addplot table [x=sp,y=run,col sep=comma] {data/laplace-sp600.csv};
\addplot table [x=sp,y=run,col sep=comma] {data/laplace-tm600.csv};
\legend{Space-tiling,Time-tiling}
\end{axis}
\end{tikzpicture}
\caption{Space-tiling and time-tiling runtimes, with a grid size of \(600^3\) and 16 time iterations. The runtimes converge as the space-order increases.}
\label{fig:laplace-graph}
\end{figure}

Table~\ref{tab:laplace-results} shows the runtimes arising from running time-tiling compared to spatial tiling under the Laplace operator.
Two grid sizes were chosen in accordance with our discussion on domain size above.

Immediately evident is that the benefit from time-tiling rapidly tapers as the space-order increases, with usage of broader stencils.
There are two likely (related) explanations for this: an increase in the amount of data needed to compute any one tile, and an increase in arithmetic intensity, slowing the rate of data consumption, making memory bandwidth less relevant.
Table~\ref{tab:laplace-gflops} shows a calculation achieved memory transfer rates.

\begin{table}[!ht]
\centering
\begin{tabular}{rcccc}
\toprule
Space-order & Tiling & Arith. intensity & Ach. GFlops & Memory transfer \\
\midrule
2 & Space & 4.69 & 39.49 & 8.42 GB/s \\
2 & Time & 75.05 & 50.80 & 10.8 GB/s \\
\midrule
4 & Space & 6.31 & 54.91 & 8.70 GB/s \\
4 & Time & 50.5 & 66.44 & 10.5 GB/s \\
\midrule
8 & Space & 9.29 & 74.06 & 7.97 GB/s \\
8 & Time & 149 & 82.97 & 8.93 GB/s \\
\midrule
16 & Space & 15.81 & 93.24 & 5.90 GB/s \\
16 & Time & 15.81 & 94.19 & 5.96 GB/s \\
\bottomrule
\end{tabular}
\caption{Memory transfer rates for different space-orders under spatial and time-tiling, grid size \(16 \times 600^3\). Arithmetic intensity is the number of floating-point operations performed per byte of data used. Achieved GFlops reported by Devito. Memory transfer rate obtained by dividing the latter by the former.}
\label{tab:laplace-gflops}
\end{table}

\begin{figure}[!ht]
\centering
\begin{tikzpicture}
\begin{axis}[
legend style={at={(0.05,0.95)},anchor=north west},
xlabel={Arithmetic intensity},
ylabel={GFlops achieved},
ytick distance=25,
]
\addplot table [x=oi,y=gflops,col sep=comma] {data/laplace-roof-sp600.csv};
\addplot table [x=oi,y=gflops,col sep=comma] {data/laplace-roof-tm600.csv};
\legend{Space-tiling,Time-tiling}
% roofline
\addplot [domain=2:15.15,name path=A] {17.3*x};
\addplot [domain=15.15:152] {262.01};
\end{axis}
\end{tikzpicture}
\caption{Graph of performance against arithmetic intensity for the Laplace operator, grid size \(16 \times 600^3\). As predicted by the roofline model, GFlops achieved increases for stencils of lower arithmetic intensity, but tapers off as arithmetic increases. The solid line is the theoretical maximum predicted by the roofline model.}
\label{fig:laplace-roofline}
\end{figure}

It is fairly clear that the achieved memory transfer rates decreases progressively as the space-order increases.
We also note that arithmetic intensity increases significantly from a space-order of 2 to 16.
At the same time, reported GFlops increase from under half of the 130 GFlops achieved by the \emph{LINPACK} benchmark, to nearly 75\%.
Therefore, we expect that we are nearing the computational bound of our test machine, and in accordance with the roofline model, would expect little improvement from memory bandwidth improvements such as time-tiling.

% Discuss peak memory bandwidth, benchmarked bandwidth, and how much Devito used
% Show that I am memory bounded, not cpu bounded -- ask TJ -- urgent


\section{Performance of the acoustic wave equation operator}
\label{sec:perf-awe}

\subsection{Application of the operator}
The acoustic wave equation (AWE) determines the propagation of acoustic waves, describing velocity as a function of space and time.
In our evaluation, we again used three spatial dimensions and a time dimension, with deterministically generated input.

In this section, the family of stencils generated by Devito are the acoustic wave equation operators governing wave propagation only,\footnote{Not including sources and receivers, required for some applications.} with space orders 4, 6, 8, 12, and 16.

\subsection{Results}

\begin{table}[!ht]
\centering
\begin{tabular}{rr|ccccc}
\toprule
& Grid size & \multicolumn{5}{c}{\( 512^3 \)} \\
& Space-order & 4 & 6 & 8 & 12 & 16 \\
\midrule
S & result (s) & 4.558 & 4.595 & 4.655 & 4.892 & 5.608 \\
T & result (s) & 2.492 & 2.688 & 2.941 & 3.660 & 4.468 \\
& decrease (\%) & 45.3\% & 41.5\% & 26.8\% & 25.2\% & 20.3\% \\
\bottomrule
\end{tabular}
\caption{Runtimes from using the acoustic wave equation operator. N, S, T represent minimum runtimes without tiling, with spatial tiling, and with time-tiling respectively. t,\(n\) indicates results for time-tiling with a skewing factor of \(n\); T indicates the minimum of these. Decrease is from spatial tiling to time-tiling. Only valid skewing factors used.}
\label{tab:awe-results}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{rcccc}
\toprule
Space-order & Tiling & Arith. intensity & Ach. GFlops & Memory transfer \\
\midrule
4 & Space & 2.10 & 27.57 & 13.1 GB/s \\
4 & Time & 2.10 & 50.44 & 24.0 GB/s \\
\midrule
8 & Space & 2.89 & 37.12 & 12.8 GB/s \\
8 & Time & 2.89 & 58.76 & 20.3 GB/s \\
\midrule
16 & Space & 4.42 & 47.10 & 10.7 GB/s \\
16 & Time & 4.42 & 5.911 & 13.4 GB/s \\
\bottomrule
\end{tabular}
\caption{Memory transfer rates for different space-orders under spatial and time-tiling, grid size \(512^3\). Arithmetic intensity is the number of floating-point operations performed per byte of data used. Achieved GFlops reported by Devito. Memory transfer rate obtained by dividing the latter by the former.}
\label{tab:awe-gflops}
\end{table}

\wip{Maybe I miscompiled STREAM?}

\wip{Text analysing the results}

Significant benefit, especially at lower arithmetic intensity, as predicted by the roofline model.
Note that the achieved GFlops seem to taper off quickly. \clar{Why?}

\begin{figure}[!ht]
\centering
\begin{tikzpicture}
\begin{axis}[
legend style={at={(0.95,0.05)},anchor=south east},
xlabel={Arithmetic intensity},
ylabel={GFlops achieved},
ytick distance=10,
]
\addplot table [x=oi,y=gflops,col sep=comma] {data/awe-roof-sp512.csv};
\addplot table [x=oi,y=gflops,col sep=comma] {data/awe-roof-tm512.csv};
\addplot [domain=2:5] {17.3*x};
\legend{Space-tiling,Time-tiling}
\end{axis}
\end{tikzpicture}
\caption{Graph of performance against arithmetic intensity for the acoustic wave equation operator, grid size \(512^3\). As predicted by the roofline model, GFlops achieved increases for stencils of lower arithmetic intensity, but tapers off as arithmetic increases. The solid line is the theoretical maximum predicted by the roofline model.}
\label{fig:awe-roofline}
\end{figure}


\section{Effect of the skewing factor}
\paragraph{Arithmetic intensity}
We do not propose changing the measure of arithmetic intensity of a stencil computation when skewing, as the same data is required for each computation, and the ordering of computations has not changed.\footnote{For a more concrete understanding of this, one may appeal to the polyhedral model which is beyond the scope of this work.}
Nevertheless, due to data non-alignment with odd skewing factors, skewing \emph{may} have the impact of reducing the effective arithmetic intensity, especially if there is a memory bandwidth bottleneck.
We believe that this is overwhelmingly offset by the increase due to time-tiling.\footnote{The reader observes that time-tiling is intended to increase (minimally double) the arithmetic intensity, and a data alignment issue will not reduce the arithmetic intensity by more than half.}

Empirically, observe that in all but one case time-tiling with a smaller skewing factor produces a lower runtime than a larger skewing factor, and that there is a progressive increase in runtime as the skewing factor increases.
This held for our test machine even if the skewing factor was not a power of two, such as 3 or 6, which were compared against skewing factors of 4 and 8 respectively.

However, it is important to note that this may be architecture-dependent, and should be tested on other architectures rather than assumed.
Further, we have only tested against 10 members across two families of stencils, which may not be fully representative of stencils in general.


\section{Limitations of performance evaluation}
\label{sec:perf-limitations}
% "Why I could be wrong/this isn't representative of real world problems"

\begin{itemize}
	\item No time buffering (Laplace) -- not the same memory accesses?
	\item Limited, small (?) grids, few time iterations
	\item Didn't collect data on cache misses...
	\item \clar{What else?}
\end{itemize}


\section{Further evaluation}
\label{sec:further-eval}

\wip{Consider moving to Conclusion}

For a more in depth analysis, we would consider memory analyses measuring cache misses and memory traffic, to ensure that improvements are indeed gleaned from time-tiling rather than other factors.

\begin{itemize}
	\item More stencils + sizes, everything else covered above
	\item Source and receiver loops
	\item \clar{More?}
\end{itemize}


\section{Conclusion}
\label{sec:eval-conclusion}
