\documentclass[thesis.tex]{subfile}

\chapter{Evaluation}
\label{ch:evaluation}

This chapter is devoted to the evaluation of performance of time-tiling as implemented in Chapter~\ref{ch:implementation}.
We discuss the functionality of the time-tiling implementation and its technical requirements (Section~\ref{sec:time-tiling-reqs}), and the test methodology (Section~\ref{sec:test-method}).
Section~\ref{sec:perf-laplace} examines results under the Laplacian operator.
Finally, we discuss limitations of testing (Section~\ref{sec:perf-limitations}), and consider further work in evaluation (Section~\ref{sec:further-eval}).


\section{Motivation}
The motivation for this project is to reduce computational runtime by exploiting data locality through time-tiling.
We have described the motivation and investigated tools which have achieve performance gains from the technique.
It had previously been shown that Devito benefits from reductions in runtime of up to 27.5\% with time-tiling using CLooG.

Chapter~\ref{ch:implementation} described our implementation of time-tiling in Devito.
The objectives of this evaluation are as follows:

\begin{itemize}
	\item Measure the performance gain against spatially-tiled computation;
	\item Verify any requirements to perform time-tiling effectively;
	\item Determine procedures and heuristics to identify the best-performing parameters for time-tiling;
	\item Additional verification of correctness, and margin of error introduced against non-tiled code\footnote{For example, due to non-associativity of floating-point arithmetic.}, if any.
\end{itemize}


\section{Pre-requisites for time-tiling}
\label{sec:time-tiling-reqs}

As time-tiling has been implemented within Devito, there is no need for any external tool such as a polyhedral compiler.
Nevertheless, some attention must be paid to configuration when using time-tiling, as follows.

\subsection{Time-buffering}
% Time buffering and save=n>2*time tile size (a nice diagram!)
Time-buffering is a memory-saving technique.

\begin{figure}[!ht]
\begin{lstlisting}
for (int t = t_s; t < t_e; t++)
  for (int x = x_s; x < x_e; x++)
    A[t][x] = A[t-1][x] + A[t-2][x] + ... + A[t-n][x];
\end{lstlisting}
	\caption{A stencil with a value depending on data in the previous \(n\) time iterations. \(n\) is usually small compared to the problem domain, (\texttt{t\_e - t\_s + n}) here.}
	\label{lst:buffer-eg}
\end{figure}

A stencil may only have dependencies on data from finitely many previous time iterations.
Consider a stencil with a data dependence on the last \(n\) time iterations (Figure~\ref{lst:buffer-eg}).
It is clear that whenever \(t > n+1\), data from the \(t=0\)-th iteration is no longer required, and can be overwritten.

When space-tiling in Devito, each time iteration only begins once the previous iteration has finished.
Therefore, we only need storage for \(n+1\) time iterations of data: the current iteration being computed; and the previous \(n\) iterations, its dependencies.

To allow for rigorous correctness checks (Section~\ref{sec:eval-func-corr}), we have \emph{not} used time-buffering during the evaluation.
Nevertheless, it is straightforward to establish bounds on the size of a time buffer.
It is clear that for time-tiling to perform better than spatial tiling, at least \(n+2\) time iterations must be stored.
\footnote{From a synchronisation perspective, this would needlessly challenging. It is a fairly simple exercise to show that time-tiling needs at most \(n+t\) time iterations of storage, where \(t\) is the size of a time tile. We would recommend this approach.
\clar{Should this be in a footnote? Need diagram/proof?}}


\subsection{Auto-tuner iterations}
% Further prereqs to use the AT
Previously covered in Section~\ref{sec:autotune}, the Devito auto-tuner experimentally searches for tile sizes achieving the lowest runtime.
To do this, it needs to store some time iterations of computations: this value is governed by a parameter.\footnote{The \texttt{at\_squeezer} parameter is the number of iterations that are computed, therefore requiring \(n + \texttt{at\_squeezer}\) time iterations of storage.}

For its results to be meaningful, we need to compute enough time iterations to distinguish between large time tile sizes.
Without time-tiling, a fairly small number of time iterations (4) sufficed for auto-tuning.
In our evaluation, we decided that the largest desirable time tile size was 16, to which we set the parameter.

\subsection{Time-tiling parameters}
\wip{What arguments; define skewing factor; redundant? discussed later}

Time-tiling provides more parameters: a valid skewing factor, and a tile size for the time dimension.
The tile size search was integrated into the auto-tuner, as it already had functionality to search for spatial tile sizes.
Arguments on data alignment give that the minimum valid skewing factor would produce the lowest runtime computation.
It was decided to try skewing factors by hand, as their validity would depend on the stencil (Section TODO).
We have found that that the data alignment argument holds for the experiments that we have performed (see Appendix TODO for the complete results).


\section{Testing methodology}
\label{sec:test-method}

Realistic test cases vary from those that are more memory-intensive to those that are more computationally intensive on a given hardware configuration.
Under the former regime, computational (arithmetic) intensity is fairly low, and the CPU uses data faster than it can be transferred from memory; in the latter case, data can be transferred more rapidly than it can be utilised.

A key premise of Devito is that arithmetic intensity can be decreased at the cost of higher memory pressure, by manipulating expressions~\cite{fabio-memory}.
Time-tiling reduces memory pressure by increasing reuse of cached data before it is evicted.
Accepting the premise that arithmetic intensity can be reduced sufficiently, time-tiling can be used to full effect in reducing memory pressure.\footnote{See Section~\ref{sec:roofline-intro} (``Roofline model'') for more detail.}
Therefore, we will mainly explore test cases which are already bound by memory throughput.

\subsection{Hardware and software environment}
Evaluation was performed on a machine equipped with a single Intel Xeon\textregistered E5-2470 operating at 2.30 GHz, with 8 cores and 16 threads, and 64 GB of DRAM.
It ran Ubuntu 16.04 LTS, with all running services required either for the operating system or our evaluation, to minimise external effects on runtime.

\clar{Discuss cache?}

To ensure a realistic testing environment and utilise all the available resources, the following hold throughout the experiments:

\clar{Assume reader knows about OpenMP?}

\wip{Explicit variables, check definitions in references}
\begin{itemize}
	\item OpenMP directives enabled in Devito. Indicates that a loop should be computed in parallel (typically the body of the inner time loop).
	\item OpenMP environment variable to utilise all 16 available threads.
	\item OpenMP environment variable not to migrate threads (``thread pinning''), as well as allocate threads to different cores. Thread migration incurs a significant performance overhead, and parallelism would not help if all threads executed in serial on one core. \clar{Too brief?}
	\item Usage of the Intel C Compiler, \texttt{icc}.
	\item Devito enables the highest level of compiler optimisation and parallelism in the compiler by default.
\end{itemize}

\subsection{Use of auto-tuner}
% Usage of AT -- parameters (skewing factor, time and space tile sizes)
We were wary of assuming which tile sizes might be optimal for time-tiling, as they are not easily predicted from those optimal for spatial tiling or cache size~\cite{lam91}.
Therefore, the extended Devito auto-tuner (Section~\ref{sec:autotune}) was used to explore as many plausible tile sizes combinations as possible.

As detailed above, experimentation on the skewing factor was done manually.
An exhaustive search was performed, as the space was relatively small, and the effect on our stencils not well studied.

As a matter of expedience, the auto-tuner was run for at least 3 trials.
If these produced the same combination of tile sizes, this combination would be chosen as optimal, and run repeatedly to determine its runtime.
This obviated the need to run the (rather lengthy) auto-tuning for each data point.

\subsection{Functional correctness}
\label{sec:eval-func-corr}

In addition to new test cases, to build confidence in correctness of the newly-implemented skewing and tiling transformations, each application of time-tiling was numerically verified against a non-tiled computation.

\wip{Keep this box until results finalised: result equality claim}

In all experiments, the results were discovered to be equal.\footnote{We refer to equality under floating-point comparison; this is \emph{bitwise equality}.}
Since floating-point arithmetic is non-associative, the natural implication is that the resulting value in each field was reached through the same expressions, although the computations had been re-ordered.
Additional checks were used to determine that the results were not merely zeroes, or diverging to infinity.

\subsection{Roofline model}
\label{sec:roofline-intro}

The roofline model describes how arithmetic (or \emph{operational}) intensity kernel affects its performance on a given system.
In particular, it gives upper bounds for performance, and describes the bottlenecks that a program would encounter based on its arithmetic intensity.~\cite{roofline}.
The model states that as arithmetic intensity increases, performance increases at a rate determined by the memory bandwidth, until it reaches the performance limit (in Flops, floating-point operations per second) of the processor.

\wip{Diagram}

Time-tiling is an optimisation to improve the re-use of cache data, increasing the effective memory bandwidth of a system.
Therefore, its benefit will only be seen under a sufficiently low arithmetic intensity, and will taper off as we approach the performance ceiling of the processor.

The Sandy Bridge architecture supports execution of up to 16 single-precision floating point operations per cycle, giving a theoretical maximum of 294.4 GFlops.
The \emph{LINPACK} benchmark, highly optimised and likely to perform faster than any stencil we evaluate, achieves a maximum of 131 GFlops on this machine.
In particular, to avoid approaching this limit, we intend to focus on stencils achieving performance well below this figure.
The \emph{STREAM} benchmark indicates that the peak memory bandwidth of the machine is 16.3 GB/s.


\subsection{Reporting of runtimes}
Wherever runtimes have been collected in this evaluation, the reported figure will be the \emph{minimum} of the runtimes collected in trials.
This is taken as most representative, as any noise on the testing machine is minimised, since outside factors can only increase the runtime of our computations.


\section{Test parameters}
\paragraph{Skewing factor}
The skewing factor was varied to understand the relationship between skewing factor and runtime.
In particular, we wished to determine if the minimum legal skewing factor would result in the minimum runtime as hypothesised.

\paragraph{Space order}
The space order of the computation determines the precision of the result.
In solving differential equations, is the order of the approximation, beyond which smaller terms are discarded.
A higher space order means each cell is computed using more of its neighbours, resulting in greater precision.
This would require a larger stencil, and hence a larger skewing factor.

\paragraph{Domain size}
The size of the domain is the product of all four dimensions in the simulation.
As memory was limited to 64GB needed to store both time-tiled results and non-tiled results for numerical verification, we decided to use shorter time dimensions, maximising the spatial dimension size for realism.
We justify this choice, reasoning that runtime should be proportional to the number of time tiles executed.\footnote{This was verified using smaller spatial dimensions.}
Therefore, we chose time dimensions 16 or 32, as these would be small multiples of the time tile size.


\section{Performance under the Laplace operator}
\label{sec:perf-laplace}
\clar{Confirm drop of \( 16 \times 400^3 \) cases}

\subsection{Application of the operator}
The Laplacian is the operator giving the divergence of the gradient of a scalar function, commonly used in mechanics.
In our evaluation, we used three spatial dimensions and a time dimension, with a deterministically generated input domain for numerical verification.
The input domain was chosen to avoid divergence of values, again for the purposes of numerical verification.
% Sample code from apply.py
% Insert perf1.py into appendix (invocation of the operator, testing rigour)

\subsection{Results}
% Sample program output

\begin{table}[!ht]
\centering
\begin{tabular}{rr|cccc|cccc}
\toprule
& Grid size & \multicolumn{4}{c}{\( 32 \times 500^3 \)} & \multicolumn{4}{c}{\( 16 \times 600^3 \)} \\
& Space-order & 2 & 4 & 8 & 16 & 2 & 4 & 8 & 16 \\
\midrule
N & \footnotesize runtime (s) & 6.121 & 7.572 & 10.65  & 16.941 & 5.189 & 6.402 & 9.026 & 14.87 \\
S & \footnotesize runtime (s) & 3.964 & 3.760 & 4.060 & 5.778 & 3.489 & 3.376 & 3.684 & 4.980 \\
% & tile size & 32,64 & 128,8 & 128,8 & 16,16 & 32,40 & 32,32 & 8,64 & 16,16 \\
T & \footnotesize runtime (s) & 2.939 & 2.957 & 3.762 & 5.718 & 2.712 & 2.790 & 3.288 & 4.929 \\
& \footnotesize decrease (\%) & 25.9\% & 21.4\% & 7.34\% & 1.04\% & 22.3\% & 17.4\% & 10.8\% & 1.02\% \\

\midrule
t,8 & \footnotesize runtime (s) & 3.348 & 3.257 & 3.762 & 5.718 & 3.089 & 2.966 & 3.341 & 4.929 \\
% & tile size & 16,32,64 & 16,32,64 & 16,32,64 & 1,16,16 & 16,64,24 & 16,32,64 & 16,16,128 & 1,16,32 \\
t,4 & \footnotesize runtime (s) & 3.114 & 3.090 & 3.764 & - & 2.844 & 2.811 & 3.288 & - \\
% & tile size & 16,32,64 & 16,32,64 & 8,16,16 & & 16,48,24 & 8,16,16 & 16,32,64 & \\
t,2 & \footnotesize runtime (s) & 2.956 & 2.957 & - & - & 2.788 & 2.790 & - & - \\
% & tile size & 16,32,64 & 16,32,64 & & & 8,16,32 & 8,16,16 & & \\
t,1 & \footnotesize runtime (s) & 2.939 & - & - & - & 2.712 & - & - & - \\
% & & 16,32,64 & & & & 16,32,32 & & & \\
\bottomrule
\end{tabular}
\caption{Runtimes from using the Laplace operator. N, S, T represent minimum runtimes without tiling, with spatial tiling, and with time-tiling respectively. t,\(n\) indicates results for time-tiling with a skewing factor of \(n\); T indicates the minimum of these. Decrease is from spatial tiling to time-tiling. Only valid skewing factors used.}
\label{tab:laplace-results}
\end{table}

\begin{figure}[!ht]
\centering
\begin{tikzpicture}
\begin{axis}[
legend style={at={(0.95,0.05)},anchor=south east},
xlabel={Space order},
ylabel={Runtime(s)},
ytick distance=0.5,
]
\addplot table [x=sp,y=run,col sep=comma] {data/laplace-sp600.csv};
\addplot table [x=sp,y=run,col sep=comma] {data/laplace-tm600.csv};
\legend{Space-tiling,Time-tiling}
\end{axis}
\end{tikzpicture}
\caption{Space-tiling and time-tiling runtimes, with a grid size of \(600^3\) and 16 time iterations. The runtimes converge as the space-order increases.}
\label{fig:laplace-graph}
\end{figure}

Table~\ref{tab:laplace-results} shows the runtimes arising from running time-tiling compared to spatial tiling under the Laplace operator.
Two grid sizes were chosen in accordance with our discussion on domain size above.

Immediately evident is that the benefit from time-tiling rapidly tapers as the space-order increases, with usage of broader stencils.
There are two likely (related) explanations for this: an increase in the amount of data needed to compute any one tile, and an increase in arithmetic intensity, slowing the rate of data consumption, making memory bandwidth less relevant.
Table~\ref{tab:laplace-gflops} shows a calculation achieved memory transfer rates.

\begin{table}[!ht]
\centering
\begin{tabular}{rcccc}
\toprule
Space-order & Tiling & Arith. intensity & Ach. GFlops & Memory transfer \\
\midrule
2 & Time & 4.69 & 50.80 & 10.8 GB/s \\
2 & Space & 4.69 & 39.49 & 8.42 GB/s \\
\midrule
4 & Time & 6.31 & 66.44 & 10.5 GB/s \\
4 & Space & 6.31 & 54.91 & 8.70 GB/s \\
\midrule
8 & Time & 9.29 & 82.97 & 8.93 GB/s \\
8 & Space & 9.29 & 74.06 & 7.97 GB/s \\
\midrule
16 & Time & 15.81 & 94.19 & 5.96 GB/s \\
16 & Space & 15.81 & 93.24 & 5.90 GB/s \\
\bottomrule
\end{tabular}
\caption{Memory transfer rates for different space-orders under spatial and time-tiling, grid size \(16 \times 600^3\). Arithmetic intensity is the number of floating-point operations performed per byte of data used. Achieved GFlops reported by Devito. Memory transfer rate obtained by dividing the latter by the former.}
\label{tab:laplace-gflops}
\end{table}

\begin{figure}[!ht]
\centering
\begin{tikzpicture}
\begin{axis}[
legend style={at={(0.95,0.05)},anchor=south east},
xlabel={Arithmetic intensity},
ylabel={GFlops achieved},
ytick distance=10,
]
\addplot table [x=oi,y=gflops,col sep=comma] {data/laplace-roof-sp600.csv};
\addplot table [x=oi,y=gflops,col sep=comma] {data/laplace-roof-tm600.csv};
\addplot [domain=4:8] {16.3*x};
\addplot [domain=8:16] {130.4};
\legend{Space-tiling,Time-tiling}
\end{axis}
\end{tikzpicture}
\caption{Graph of performance against arithmetic intensity for the Laplace operator, grid size \(16 \times 600^3\). As predicted by the roofline model, GFlops achieved increases for stencils of lower arithmetic intensity, but tapers off as arithmetic increases. The solid line is the theoretical maximum predicted by the roofline model.}
\label{fig:laplace-roofline}
\end{figure}

It is fairly clear that the achieved memory transfer rates decreases progressively as the space-order increases.
We also note that arithmetic intensity increases significantly from a space-order of 2 to 16.
At the same time, reported GFlops increase from under half of the 130 GFlops achieved by the \emph{LINPACK} benchmark, to nearly 75\%.
Therefore, we expect that we are nearing the computational bound of our test machine, and in accordance with the roofline model, would expect little improvement from memory bandwidth improvements such as time-tiling.

% Discuss peak memory bandwidth, benchmarked bandwidth, and how much Devito used
% Show that I am memory bounded, not cpu bounded -- ask TJ -- urgent


\section{Performance under the acoustic wave equation operator}
\label{sec:perf-awe}
\wip{Name this section something shorter}

\subsection{Application of the operator}
The acoustic wave equation (AWE) determines the propagation of acoustic waves, describing velocity as a function of space and time.
In our evaluation, we again used three spatial dimensions and a time dimension, with deterministically generated input.

\subsection{Results}


\section{Effect of the skewing factor}
We observe that in almost every case, time-tiling with a smaller skewing factor produces a lower runtime than a larger one, and that there is a progressive increase in runtime as the skewing factor increases.

\wip{Discuss odd skewing factors, e.g. k=3}

However, we would be wary of drawing this conclusion for stencils under Devito in general, until more stencils have been sampled.


\section{Limitations of performance evaluation}
\label{sec:perf-limitations}
% "Why I could be wrong/this isn't representative of real world problems"

% Ask Paul/Fabio
% No time buffering, so not the same memory accesses
% Only ran X trials
% Didn't run vtune, can't prove -why- we're faster. See Dylan 6.7


\section{Further evaluation}
\label{sec:further-eval}

% Bigger time tiles
% Time buffering (since we store everything to verify)
% More grid sizes!
% Try more stencils (eqns etc)
% Try bigger grids

For a more in depth analysis, we would consider memory analyses measuring cache misses and memory traffic, to ensure that improvements are indeed gleaned from time-tiling rather than other factors.


\section{Conclusion}
\label{sec:eval-conclusion}
