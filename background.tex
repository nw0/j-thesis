\documentclass[thesis.tex]{subfile}

% Purpose of background chapter
% 1. Background to understand thesis (short)
% 2. "Related work", or intro++: research context, where the boundaries are

% Examiners will know about: PDEs, maybe handcoding

\chapter{Background and related work}

% TODO: \ref sections here
We first provide an overview of loop nest optimisations, techniques, and analyses that we have applied, with specific reference to the polyhedral model. % TODO: cite
This forms the basis and context for the entire report, and in particular the survey of related work, which composes the remainder of this chapter.

% TODO: grandiose
This project extends a well-established idea from compiler theory, \emph{tiling}, to another dimension (time) in Devito.
This has traditionally been a challenging problem, as evaluating data dependences efficiently is beset with difficulties.

% We have a tradeoff between computation and memory usage
% Big IDEA: can use more memory, less computation ("this really should be true")
% Project: optimise memory usage
% Why exciting: one step to proving the idea


\section{Loop tiling}
% TODO: mention that iteration spaces under consideration are (nice rectangular continuous) matrices

\subsubsection{Optimisations on loop nests}
The bulk of computation for finite difference methods lies in loops. % TODO: cite
Loop nest optimisations seek to transform a loop, possibly changing its execution order to use data locality, parallelism, or otherwise avoid unnecessary operations.

% TODO
\paragraph{Data locality}
Something about caches: yet another thing I don't know how to explain...

\subsection{Insight}
To exploit data locality, we must use data before it gets evicted from the cache; ideally, data is not loaded into the cache more than once.
This is a complex process \cite{lam91}; nevertheless reuse does occur within sufficiently small iteration spaces.
We therefore contrive small iteration spaces by partitioning the original space into smaller tiles (Figure~\ref{fig:tiled-space}).

\begin{figure}[h]
	\centering
	\textbf{[A nice picture of some tiles]}
	\caption{Tiles over an iteration space. Note that the tile size need not be the same in each dimension, or divide the extent of the iteration cleanly.}
	\label{fig:tiled-space}
\end{figure}

Loop tiling is also commonly known as \emph{blocking}, or perhaps less transparently \emph{strip-mine and interchange}, as tiling is typically achieved through these two transformations.\footnote{Tiling may also enable other transformations, such as loop-invariant code motion, which may not be appropriate when the extents of the iteration are too large.}

\subsection{Strip-mining}
Named after the mining practice, strip-mining involves dividing a dimension of the iteration space into strips (Figure~\ref{lst:stripmine-basic}).\footnote{However, you cannot divide a dimension into lateral strips, only sequential ones.}
By itself, strip-mining does not change the execution order; it is a gateway to further transformations.

\begin{figure}[h]
\begin{lstlisting}
for (int x = x_start + 1; x < x_end - 1; x++) {
  A[x] = B[x-1] + B[x+1];
}

for (int x_blk = x_start + 1; x_blk < x_end - 1; x_blk += x_blk_size) {
  for (int x = x_blk; x < min (x_end - 1, x_blk + x_blk_size); x++) {
    A[x] = B[x-1] + B[x+1]; // loop body unchanged
  }
}
\end{lstlisting}
	\caption{A regular loop, then strip-mined over the variable \texttt{x}. Offsets are used on \texttt{x\_start}, \texttt{x\_end} to prevent out-of-bounds accesses. The \texttt{min} function avoids the need for remainder loops, in case the tile (block) size does not evenly divide the extent of the iteration. We will abbreviate the variable names in further examples.}
	\label{lst:stripmine-basic}
\end{figure}

We will need more loops to perform an interchange.
Figure~\ref{lst:stripmine} illustrates a loop that has been strip-mined in two dimensions.

\begin{figure}[h]
	\begin{lstlisting}
for (int x_blk = x_s; x_blk < x_e; x_blk += x_bs) {
  for (int x = x_blk; x < min(x_e, x_blk + x_bs); x++) {
    for (int y_blk = y_s; y_blk < y_e; y_blk += y_bs) {
      for (int y = y_blk; y < min(y_e, y_blk + y_bs); y++) {
        A[x][y] = B[x][y] + B[x][y+1];
      }
    }
  }
}
	\end{lstlisting}
	\caption{Strip-mining a loop nest iterating over variables \texttt{x} and \texttt{y}. Offsets have been omitted here.}
	\label{lst:stripmine}
\end{figure}

% TODO: consider visualising the iteration space here

\subsection{Loop interchange}
\label{sec:interchange}
Loop interchange is based on the observation that a change in execution order does not change the correctness of a strip-mined program.
We will change the order of the loops to iterate over the tiles, then within them (Figure~\ref{lst:interchange}).

\begin{figure}[h]
\begin{lstlisting}
for (int x_blk = x_s; x_blk < x_e; x_blk += x_bs) {
  for (int y_blk = y_s; y_blk < y_e; y_blk += y_bs) {
    for (int x = x_blk; x < min(x_e, x_blk + x_bs); x++) {
      for (int y = y_blk; y < min(y_e, y_blk + y_bs); y++) {
        A[x][y] = B[x][y] + B[x][y+1];
      }
    }
  }
}
\end{lstlisting}
	\caption{The loop nest of Figure~\ref{lst:stripmine}, with the \texttt{x} and \texttt{y\_blk} loops interchanged.}
	\label{lst:interchange}
\end{figure}

This is valid when each point in the iteration space does not depend on the values calculated in the same iteration.
Therefore, one must be extremely careful that no data dependences cross boundaries between tiles; if they do, they must be permitted to cross only in one direction, and the tiles must be scheduled in that order.

\subsection{Skewing}
In the previous section (\ref{sec:interchange}) we stated that interchange is valid when data dependences do not cross boundaries between tiles.
This is clear, as if there are no inter-tile dependences, the tiles can be executed in any order.

% TODO: "problems we discuss later": verify these are indeed discussed
% This could probably be clearer
\emph{Data dependences} occur when two statements reference a datum, at least one of which change it.
To preserve the dependence, we must preserve the order in which these statements are executed.
Figure~\ref{fig:dependence} illustrates how dependences may look in a (1-dimensional) iteration space similar to the problems we discuss later.

\begin{figure}[h]
	\centering
	\textbf{[Diagram with dependence arrows]}
	\caption{An iteration space with data dependences indicated by a forward arrow for a value derived from a dependence. It would not be valid to interchange loops over the \texttt{t} and \texttt{x} dimensions here.}
	\label{fig:dependence}
\end{figure}

We employ skewing to make the interchange valid (Figure~\ref{fig:dependence-skew})
This solves the dependency problem~\cite{boulet98}.

% TODO: vague
\begin{figure}[h]
	\centering
	\textbf{[Diagram (skewed) with dependence arrows]}
	\caption{The same iteration space skewed by a factor of 2 in the \texttt{t} dimension. Note that the tiling is now valid, i.e.~we can execute the tiles in either dimension first.}
	\label{fig:dependence-skew}
\end{figure}

\subsection{Polyhedral model}
[ How much do I want to discuss this, if at all?
It is very relevant, but I haven't seen polyhedra (or scanning of such) in the Devito codebase, and don't have a good grasp of them. ]


\section{Tiling in the time dimension}
% We have already discussed why time-tiling is hard
% Now we need to talk about why we want it

Time-tiling is important to realise speedup from parallelism; it has been suggested that tiling over the inner dimensions (apart from time) only results in minimal, if any, benefit from using more cores~\cite{pluto}.

% TODO: vague; so what?
Further, many problems involving finite difference methods are computationally bounded, rather than bounded by memory throughput.
However, it is possible to reduce the operation count by exploiting the structure of expressions computed at the cost of increased memory pressure~\cite{fabio-memory}.
We are therefore interested to perform time-tiling to realise significant performance gains, possibly 27.5\% in Devito alone~\cite{dylan}.


\section{Devito}
Devito~\cite{devito} is a domain-specific language and code generation framework for finite-difference method computations~\cite{devito-web}.
Its main purpose is to build solvers for differential equations from high-level mathematical expressions written using the symbolic library \emph{SymPy}.

\begin{itemize}
	\item Transform expression into stencils
	\item Construct stencil equations
	\item Generate code (either natively or YASK, etc.)
	\item For stencil applications (not every finite-difference problem)
	\item Lower level API (recall: sparse point interpolation)
\end{itemize}


\section{YASK}
\emph{Yet Another Stencil Kernel} is a framework that transforms and optimises stencil kernels written in C++, specifically targeting the Xeon Phi platform~\cite{yask-web}.

\begin{itemize}
	\item Idea: stencils can be optimised using techniques not in general-purpose compilers (e.g.~`precise permutations of SIMD registers') \cite{yask-paper}
	\item `reduce bottleneck of memory bandwidth (!) cf. CLooG, PLUTO
	\item Allows users to choose transformations (``recipes'')
	\item Input: working stencil source code (provided classes, not magic)
	\item Folding, loop generation, (similar transformations)
	\item Looks like the user might want to stare at the generated code
	\item There is also an auto-tuning tool
	\item Question: when would Devito invoke YASK?
\end{itemize}


\section{Pochoir}
\begin{itemize}
	\item Looks like YASK
	\item Periodic grids \cite{pochoir}
	\item Finding parallelism and multithreading?
	\item Safety check (so you can debug) then optimisation
	\item References the polyhedral model
	\item Is it maintained?
\end{itemize}


\section{Firedrake}
\begin{itemize}
	\item FEniCS: separate usage and implementation of FEM
	\item Firedrake: separate discretisation of operators and parallel execution (kernels vs calculations?) \cite{firedrake}
	\item Also lower level access: custom kernels, direct access to data structures
	\item More general than Devito?
\end{itemize}


\section{Halide}
\begin{itemize}
	\item For images
	\item We can separate the algorithm (``filters'') and the schedule \cite{halide12}
	\item Same notes on parallelism and loop blocking
	\item Image processing naturally memory-bound (?)
	\item Auto-tuned scheduler \cite{halide13}
	\item No worry about skewing (?)
\end{itemize}
